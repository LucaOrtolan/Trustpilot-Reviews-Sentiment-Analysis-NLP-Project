{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659f9a0b-138d-46ab-b12e-29338b65b3e5",
   "metadata": {},
   "source": [
    "# NLP HughesNet Project Webscraping Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54373541-863b-4efd-8511-e6a48151a50c",
   "metadata": {},
   "source": [
    "This notebook shows how I webscraped Trustpilot reviews of the internet company HughesNet. I have chosen to gather reviews for this company since it roughly received the same amount of positive and negative feedbacks, which makes it good data for training a ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3273c9-a641-4d39-b397-663b6a144469",
   "metadata": {},
   "source": [
    "To start let's import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6684aa3f-3347-4eb1-88e1-e91a5ece6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbdb02-f1b3-4257-98c8-6267c791c1e8",
   "metadata": {},
   "source": [
    "The first page we're going to scrape is https://www.trustpilot.com/review/hughesnet.com?page=1, in which there are 20 reviews. In total there are 3838 pages so we can expect to scrape around 70k reviews in total. Along with the text, we're also going to scrape the number of stars (from 1 to 5) for each review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a6f2e-a9ce-48c1-8ec7-992af757b75d",
   "metadata": {},
   "source": [
    "Unfortunately, Trustpilot has a bot detection system which will temporarily deny us access after a certain number of requests. We can work around it by setting a 10 minutes timer everytime our request gets denied. After that, we'll restart automatically the loop from where it left using a boolean variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a7e079-3500-4293-b081-e278aca2236c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 100 completed\n",
      "Page 200 completed\n",
      "DETECTED AT 244\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 300 completed\n",
      "Page 400 completed\n",
      "DETECTED AT 421\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 500 completed\n",
      "DETECTED AT 581\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 600 completed\n",
      "Page 700 completed\n",
      "DETECTED AT 765\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 800 completed\n",
      "Page 900 completed\n",
      "DETECTED AT 944\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 1000 completed\n",
      "Page 1100 completed\n",
      "DETECTED AT 1179\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 1200 completed\n",
      "Page 1300 completed\n",
      "DETECTED AT 1373\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 1400 completed\n",
      "Page 1500 completed\n",
      "DETECTED AT 1558\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 1600 completed\n",
      "Page 1700 completed\n",
      "DETECTED AT 1732\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 1800 completed\n",
      "Page 1900 completed\n",
      "DETECTED AT 1911\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 2000 completed\n",
      "DETECTED AT 2088\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 2100 completed\n",
      "Page 2200 completed\n",
      "DETECTED AT 2259\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 2300 completed\n",
      "Page 2400 completed\n",
      "DETECTED AT 2436\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 2500 completed\n",
      "Page 2600 completed\n",
      "DETECTED AT 2609\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 2700 completed\n",
      "DETECTED AT 2792\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 2800 completed\n",
      "Page 2900 completed\n",
      "DETECTED AT 2970\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 3000 completed\n",
      "Page 3100 completed\n",
      "DETECTED AT 3132\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 3200 completed\n",
      "Page 3300 completed\n",
      "DETECTED AT 3312\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 3400 completed\n",
      "DETECTED AT 3486\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 3500 completed\n",
      "Page 3600 completed\n",
      "DETECTED AT 3646\n",
      "RESTARTING IN 10 MINUTES\n",
      "Page 3700 completed\n",
      "Page 3800 completed\n",
      "DETECTED AT 3826\n",
      "RESTARTING IN 10 MINUTES\n"
     ]
    }
   ],
   "source": [
    "sentiment = [] # Here we'll store the number of stars\n",
    "text = [] # Here we'll store the reviews' texts\n",
    "\n",
    "for i in range(1, 3848):\n",
    "    restart = True  # We set a boolean variable to restart the loop when we'll get detected\n",
    "    while restart:\n",
    "        restart=False\n",
    "        url = f\"https://www.trustpilot.com/review/hughesnet.com?page={i}\"\n",
    "        req = requests.get(url).text\n",
    "        page = bs(req, \"html.parser\")\n",
    "\n",
    "        revs = page.find_all(\"article\") # list with all the reviews in the page (20)\n",
    "\n",
    "        if len(revs) == 0: # if the list is empty it means we have been detected\n",
    "            print(f\"DETECTED AT {i}\")\n",
    "            print(\"RESTARTING IN 10 MINUTES\")\n",
    "            time.sleep(600) # 10 mins timer\n",
    "            i-=1\n",
    "            restart=True # once the timer runs out the loop will automatically restart\n",
    "\n",
    "        for rev in revs:\n",
    "            stars = rev.find(\"div\", class_=\"styles_reviewHeader__iU9Px\") # scrape the stars\n",
    "            stars = stars[\"data-service-review-rating\"] \n",
    "            sentiment.append(stars)\n",
    "\n",
    "            txt = rev.find(\"p\").text # scrape the text\n",
    "            text.append(txt)\n",
    "\n",
    "        if i%100 == 0:\n",
    "            print(f\"Page {i} completed\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbfdc0-13ec-434b-9dec-6c53a6c8edf0",
   "metadata": {},
   "source": [
    "Done! Let's put all this data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4bc95b-d93f-4040-a565-49506b3d04b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The efforts of the Hughes net team and field t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our \"Local\" bank merged with an out of town gr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I requsted my service to be suspended while I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This rep had the knowledge and power to resolv...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Started out good ,got disconnected next day ,W...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0  The efforts of the Hughes net team and field t...         3\n",
       "1  Our \"Local\" bank merged with an out of town gr...         5\n",
       "2  I requsted my service to be suspended while I ...         5\n",
       "3  This rep had the knowledge and power to resolv...         5\n",
       "4  Started out good ,got disconnected next day ,W...         5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Text\":text, \"Sentiment\":sentiment})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41022fb-13e4-4b04-8b7c-014aed6a2027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76940 entries, 0 to 76939\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       76940 non-null  object\n",
      " 1   Sentiment  76940 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d0431-5089-4210-b9ae-5eb4f40c506b",
   "metadata": {},
   "source": [
    "Looks like we have a reasonably large amount of data and zero null values! We export it as a csv file for future using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48191eb-af69-498a-8dac-49c3e5275079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"D:/Utente/Documenti/datasets/trustpilot_nlp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18eb6f-3238-4289-8b6b-db8b6af65379",
   "metadata": {},
   "source": [
    "That's it for the webscraping part, check out the other notebook for the sentiment model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
